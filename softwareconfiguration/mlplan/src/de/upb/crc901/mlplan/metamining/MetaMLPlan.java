package de.upb.crc901.mlplan.metamining;

import java.io.File;
import java.io.IOException;
import java.util.Collection;
import java.util.List;
import java.util.Random;
import java.util.Timer;
import java.util.TimerTask;

import org.apache.commons.lang3.time.StopWatch;

import de.upb.crc901.mlplan.metamining.databaseconnection.ExperimentRepository;
import de.upb.crc901.mlplan.multiclass.wekamlplan.MLPlanWekaClassifier;
import de.upb.crc901.mlplan.multiclass.wekamlplan.weka.MLPipelineComponentInstanceFactory;
import de.upb.crc901.mlplan.multiclass.wekamlplan.weka.WEKAPipelineFactory;
import de.upb.crc901.mlplan.multiclass.wekamlplan.weka.model.MLPipeline;
import hasco.core.Util;
import hasco.metamining.MetaMinerBasedSorter;
import hasco.model.Component;
import hasco.model.ComponentInstance;
import jaicore.ml.evaluation.MonteCarloCrossValidationEvaluator;
import jaicore.ml.evaluation.MulticlassEvaluator;
import jaicore.ml.metafeatures.GlobalCharacterizer;
import jaicore.planning.graphgenerators.task.tfd.TFDNode;
import jaicore.search.algorithms.standard.AbstractORGraphSearch;
import jaicore.search.algorithms.standard.bestfirst.nodeevaluation.INodeEvaluator;
import jaicore.search.algorithms.standard.lds.BestFirstLimitedDiscrepancySearch;
import jaicore.search.algorithms.standard.lds.BestFirstLimitedDiscrepancySearchFactory;
import jaicore.search.algorithms.standard.lds.NodeOrderList;
import jaicore.search.core.interfaces.GraphGenerator;
import jaicore.search.model.other.EvaluatedSearchGraphPath;
import jaicore.search.model.probleminputs.NodeRecommendedTree;
import jaicore.search.model.travesaltree.Node;
import weka.classifiers.AbstractClassifier;
import weka.classifiers.Classifier;
import weka.core.Instance;
import weka.core.Instances;

public class MetaMLPlan extends AbstractClassifier {

	/**
	 * Generated by Eclipse
	 */
	private static final long serialVersionUID = 4772178784402396834L;

	// Search components
	private AbstractORGraphSearch<NodeRecommendedTree<TFDNode, String>, EvaluatedSearchGraphPath<TFDNode, String, NodeOrderList>, TFDNode, String, NodeOrderList, Node<TFDNode, NodeOrderList>, String> lds;
	private WEKAMetaminer metaMiner;
	WEKAPipelineFactory factory = new WEKAPipelineFactory();

	// Search configuration
	private long timeoutInMilliseconds = 60000;
	private long safety = 1000;
	private int CPUs = 1;
	private String metaFeatureSetName = "all";
	private String datasetSetName = "all";

	// Search results
	private Classifier bestModel;
	private Collection<Component> components;

	public MetaMLPlan() throws IOException {
		this(new File("model/weka/weka-all-autoweka.json"));
	}

	public MetaMLPlan(File configFile) throws IOException {
		MLPlanWekaClassifier mlPlan = new MLPlanWekaClassifier(configFile, factory, null, null) {
			@Override
			protected INodeEvaluator<TFDNode, Double> getSemanticNodeEvaluator(Instances data) {
				return null;
			}
		};
		this.components = mlPlan.getComponents();
		metaMiner = new WEKAMetaminer(mlPlan.getComponentParamRefinements());
		BestFirstLimitedDiscrepancySearchFactory<TFDNode, String, NodeOrderList> ldsFactory = new BestFirstLimitedDiscrepancySearchFactory<>();
		NodeRecommendedTree<TFDNode, String> problemInput = new NodeRecommendedTree<>(mlPlan.getGraphGenerator(),new MetaMinerBasedSorter(metaMiner, mlPlan.getComponents()));
		ldsFactory.setProblemInput(problemInput);
		this.lds = ldsFactory.getAlgorithm();
	}

	public void buildMetaComponents(String host, String user, String password) throws Exception {
		ExperimentRepository repo = new ExperimentRepository(host, user, password,
				new MLPipelineComponentInstanceFactory(components), CPUs, metaFeatureSetName, datasetSetName);
		metaMiner.build(repo.getDistinctPipelines(), repo.getDatasetCahracterizations(),
				repo.getPipelineResultsOnDatasets());
	}

	public void buildMetaComponents(String host, String user, String password, int limit) throws Exception {
		// TODO maybe some sophisticated selection for limit? / temporary method
		System.out.println("MetaMLPlan: Get past experiment data from data base and build MetaMiner.");
		ExperimentRepository repo = new ExperimentRepository(host, user, password,
				new MLPipelineComponentInstanceFactory(components), CPUs, metaFeatureSetName, datasetSetName);
		repo.setLimit(limit);
		metaMiner.build(repo.getDistinctPipelines(), repo.getDatasetCahracterizations(),
				repo.getPipelineResultsOnDatasets());
	}

	@Override
	public void buildClassifier(Instances data) throws Exception {
		// Start timer to interrupt search if it takes to long
		TimerTask tt = new TimerTask() {

			@Override
			public void run() {
				System.out.println("Interrupting search because time is running out.");
				lds.cancel();
			}
		};

		// Start timer that takes into account training time of the best model as well
		new Timer().schedule(tt, timeoutInMilliseconds - safety);
		StopWatch totalTimer = new StopWatch();
		totalTimer.start();

		// Characterize data set and give to meta miner
		System.out.println("Characterizing data set");
		metaMiner.setDataSetCharacterization(new GlobalCharacterizer().characterize(data));

		// Preparing the split for validating pipelines
		System.out.println("Preparing validation split");
		MonteCarloCrossValidationEvaluator mccv = new MonteCarloCrossValidationEvaluator(
				new MulticlassEvaluator(new Random(0)), 3, data, .7f);

		// Search for solutions
		System.out.println("Searching for solutions");
		StopWatch trainingTimer = new StopWatch();
		bestModel = null;
		double bestScore = 1;
		double bestModelExpectedTrainingTime = 0;

		while (!lds.isCanceled()) {
			List<TFDNode> solution = lds.nextSolution();
			if (solution == null) {
				System.out.println("Ran out of solutions.");
				break;
			}
			try {
				// Prepare pipeline
				ComponentInstance ci = Util.getSolutionCompositionFromState(components,
						solution.get(solution.size() - 1).getState(), true);
				MLPipeline pl = factory.getComponentInstantiation(ci);

				// Evaluate pipeline
				trainingTimer.reset();
				trainingTimer.start();
				System.out.println("Evaluate Pipeline: " + pl);
				double score = mccv.evaluate(pl);
				System.out.println("Score: " + score);
				trainingTimer.stop();

				// Check if better than previous best
				System.out.println(score + " " + pl);
				if (score < bestScore) {
					bestModel = pl;
					bestScore = score;
					bestModelExpectedTrainingTime = trainingTimer.getTime() / 2;
				}

				// Check if enough time remaining to re-train the current best model on the
				// whole training data
				if ((timeoutInMilliseconds - safety) <= (totalTimer.getTime() + bestModelExpectedTrainingTime)) {
					System.out.println(
							"Stopping search to train best model on whole training data which is expected to take "
									+ bestModelExpectedTrainingTime + " ms.");
					break;
				}
			} catch (Throwable e) {
				e.printStackTrace();
			}

			System.out.println("Evaluating best model with validation score " + bestScore + " on whole training data ("
					+ bestModel + ")");
			bestModel.buildClassifier(data);
		}

		System.out.println("Meta MLPlan ready. Best solution: " + bestModel);
	}

	@Override
	public double classifyInstance(final Instance instance) throws Exception {
		return bestModel.classifyInstance(instance);
	}

	public void setTimeOutInMilliSeconds(int seconds) {
		this.timeoutInMilliseconds = seconds * 1000;
	}

	public void setMetaFeatureSetName(String metaFeatureSetName) {
		this.metaFeatureSetName = metaFeatureSetName;
	}

	public void setDatasetSetName(String datasetSetName) {
		this.datasetSetName = datasetSetName;
	}

	public void setCPUs(int cPUs) {
		CPUs = cPUs;
	}
}
